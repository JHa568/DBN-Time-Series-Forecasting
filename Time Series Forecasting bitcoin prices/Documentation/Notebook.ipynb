{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Time Series Prediction Of Bitcoin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Objective*:\n",
    " - To create a neural network that can do time series predictions on bitcoin trends.\n",
    " - *NOTE*: It is ok for the predictions to be a off at a certain tolerence threshold however, they must predict the future trends relieably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture:\n",
    "The neural network architecture is a DBN *(Deep Belief Network)* utilises two RBMs *(Restricted Boltsmann Machine)* and one MLP *(Multi-Layer Perceptron)*. \n",
    "\n",
    "#### &emsp;RBM's Mechanism:\n",
    "  &emsp;&emsp;The RBM works like this...\n",
    "\n",
    "#### &emsp;MLP's Mechanism:\n",
    "> The Multi-Layer Perceptron is one of the most common neural network architechture. Due to its common usage, it is the easiest neural network architecture to understand and produce.\n",
    "<img src=\"img/MLP.jpg\" style=\"width:300px;height:200px\"/>\n",
    "> The mechanism begins with the inputs that are feed into the input layer, which then make a chain reaction that feed towards the output. The nodes in the hidden layer are calculated by the multiple of the input nodes($inputLayer_{n}$; where n is the number of nodes) and weights ($weight0_{n}$; *they are the connections that bond the layers together* | In this instance the layers that needed to be connected are $input_{n}$ ($layer0_{n}$) and $hiddenLayer_{n}$ ($layer1_{n}$) |).\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $$x = layer0_{n}weight0_{n}$$\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$x = layerN_{n}weightN_{n}$$ \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;*- Where $N$ is the position of the layer in the neural network architecture.*\n",
    "<br>\n",
    "<br>\n",
    "> Then the matrix product is feed through an activation function; there are many different types, however in this example the activation function is a sigmoid function. Let the activation function is notated by $a(x)$; where $x$ is equal to $layer0_{n}weight0_{n}$. This produces the nodes from the hidden layer.\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $$a(x)= \\frac{1}{1+e^x}$$\n",
    "<br>\n",
    "> This process continues until it reaches the the output layer. \n",
    "> #### Training MLP:\n",
    "> > A method of training this neural network is backpropagation; it essentially changes the weights by propagating the error backwards.\n",
    "> > Steps of backpropagation:\n",
    "> > > **1.** Compute the output error using a cost function (e.g. *mean squared error*)\n",
    "<br>\n",
    "&emsp;&emsp;&emsp; $$-([intendedOutput_n] - [actualOutput_n])$$ \n",
    "<br>*(change this later)*<br>\n",
    "<br>\n",
    "> > > **2.** The error preduced previously is then multiplied by the outputlayer that has been feed through the derivative of the activation function. \n",
    "<br>\n",
    "&emsp;&emsp;&emsp; $$(([intendedOutput_n] - [actualOutput_n]) * a'([outputLayer_n]))$$\n",
    "<br>\n",
    "> > > **3.** The product computed by the previous step is them multiplied by the hiddenLayer (*the layer that goes before the output* notated by $[prevLayer_n]$).\n",
    "<br>\n",
    "&emsp;&emsp; $$[errorProduct_n] = (([intendedOutput_n] - [actualOutput_n]) * a'([outputLayer_n]))*[prevLayer_n]$$\n",
    "<br>\n",
    "> > > **4.** The matrix $errorProduct_n$ is then transposed and then multiplied by the $weightN_n$ (that connected to another layer).\n",
    "<br>\n",
    "<br>\n",
    "     Let $N$ equal to the ***position of weight or layer***.\n",
    "<br>\n",
    "<br>\n",
    "$$[errorHiddenLayerNOutput_{x,y}] = [errorProduct_x]^t * [weightN_{x,y}]$$\n",
    "<br>\n",
    "> > > **5.** The $[errorHiddenLayereNOutput_{x,y}]$ is then broken down into a one dimensional array $[deltaHiddenLayerNOutput_x]$; produced by taking the sum each column and storing them in the $[deltaHiddenLayerNOutput_x]$. \n",
    "<br>\n",
    "<br>\n",
    "     Let $h$ equal to the ***Number of the nodes in hidden layer N***. <br>\n",
    "     Let $o$ equal to the ***Number of the nodes in the output layer***. <br>\n",
    "     Let $n$ equal to the ***Number of nodes in total***. \n",
    "<br>\n",
    "$$[deltaHiddenLayerNOutput_x] = \\sum_{h=0}^{n}(\\sum_{o=0}^{n}[errorHiddenLayereNOutput_{h,o}])$$\n",
    "<br>\n",
    "> > > **6.** Finally to update the weights (*notated by* $[deltaWeightOnHiddenLayerN_x]$) where it composes of the product of $[deltaHiddenLayerNOutput_x]$, $a'([HiddenLayerN_x])$, and $[HiddenLayerN-1_x]$.\n",
    "<br>\n",
    "\n",
    "$$[deltaWeightOnHiddenLayerN_x] = [deltaHiddenLayerNOutput_x] * a'([HiddenLayerN_x]) * [HiddenLayer(N-1)_x]$$\n",
    "> > > **7.** Repeat steps 5 and 6 until it reaches the input values. When it begins processing the last matrix of weights where instead: \n",
    "<br>\n",
    "  - Of multiplying by $[HiddenLayer(N-1)_x]$, it multiplies by $[inputLayer_x]$.<br>\n",
    "  - Of Step 5 it just places the original inputs. \n",
    "\n",
    "$$[deltaWeightOnHiddenLayerN_x] = [deltaHiddenLayerNOutput_x] * a'([HiddenLayerN_x]) * [inputLayer_x]$$\n",
    "> > > **8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Mechanism:\n",
    "The learning mechanism to train the neural network is reinforcement learning, however it is kinda uses a similar technique to supervised learning in updating the weights. This is due to the conventional reinforcement learning algorithm outputs values between 0 and 1.\n",
    "\n",
    "#### &emsp;Basics of Reinforcement Learning:\n",
    "> The reinforcement learning algorithm is composed of the *Agent* and the *Environment*, where the *Agent* reacts to the current state of the *Environment* which in return the agent receives a *reward*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- [Reinforcement Learning in Time Series Prediction](https://www.intechopen.com/online-first/training-deep-neural-networks-with-reinforcement-learning-for-time-series-forecasting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
